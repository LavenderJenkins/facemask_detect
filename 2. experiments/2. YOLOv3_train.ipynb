{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50ef840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC_DIR: D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\3. src\n",
      "TRAIN_IMG_DIR: D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\1.Dataset\\PWMFD_YOLO\\train\\images\n",
      "VAL_IMG_DIR  : D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\1.Dataset\\PWMFD_YOLO\\val\\images\n",
      "CKPT_DIR     : D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\4. Checkpoints\\yolov3\n",
      "LOG_DIR      : D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\5. Results\\1. logs\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "\n",
    "# ========== PATHS ==========\n",
    "PROJECT_ROOT = r\"D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\"\n",
    "SRC_DIR = os.path.join(PROJECT_ROOT, \"3. src\")\n",
    "EXP_DIR = os.path.join(PROJECT_ROOT, \"2. experiments\")\n",
    "\n",
    "DATA_ROOT = os.path.join(PROJECT_ROOT, \"1.Dataset\", \"PWMFD_YOLO\")  # sửa nếu tên khác\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_ROOT, \"train\", \"images\")\n",
    "TRAIN_LAB_DIR = os.path.join(DATA_ROOT, \"train\", \"labels\")\n",
    "VAL_IMG_DIR   = os.path.join(DATA_ROOT, \"val\", \"images\")\n",
    "VAL_LAB_DIR   = os.path.join(DATA_ROOT, \"val\", \"labels\")\n",
    "\n",
    "CKPT_DIR = os.path.join(PROJECT_ROOT, \"4. Checkpoints\", \"yolov3\")\n",
    "LOG_DIR  = os.path.join(PROJECT_ROOT, \"5. Results\", \"1. logs\")\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# ========== sys.path ==========\n",
    "if SRC_DIR not in sys.path:\n",
    "    sys.path.append(SRC_DIR)\n",
    "\n",
    "print(\"SRC_DIR:\", SRC_DIR)\n",
    "print(\"TRAIN_IMG_DIR:\", TRAIN_IMG_DIR)\n",
    "print(\"VAL_IMG_DIR  :\", VAL_IMG_DIR)\n",
    "print(\"CKPT_DIR     :\", CKPT_DIR)\n",
    "print(\"LOG_DIR      :\", LOG_DIR)\n",
    "\n",
    "# ========== DEVICE ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ffe35c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt: D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\1.Dataset\\PWMFD_YOLO\\train.txt\n",
      "val.txt  : D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\1.Dataset\\PWMFD_YOLO\\val.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATASET_ROOT = r\"D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\1.Dataset\"\n",
    "YOLO_ROOT = os.path.join(DATASET_ROOT, \"PWMFD_YOLO\")\n",
    "\n",
    "train_img_dir = os.path.join(YOLO_ROOT, \"train\", \"images\")\n",
    "val_img_dir   = os.path.join(YOLO_ROOT, \"val\", \"images\")\n",
    "\n",
    "train_txt = os.path.join(YOLO_ROOT, \"train.txt\")\n",
    "val_txt   = os.path.join(YOLO_ROOT, \"val.txt\")\n",
    "\n",
    "with open(train_txt, \"w\") as f:\n",
    "    for name in sorted(os.listdir(train_img_dir)):\n",
    "        if name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            f.write(os.path.join(train_img_dir, name) + \"\\n\")\n",
    "\n",
    "with open(val_txt, \"w\") as f:\n",
    "    for name in sorted(os.listdir(val_img_dir)):\n",
    "        if name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            f.write(os.path.join(val_img_dir, name) + \"\\n\")\n",
    "\n",
    "print(\"train.txt:\", train_txt)\n",
    "print(\"val.txt  :\", val_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d206a328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN images: 5908 | labels: 5908\n",
      "VAL   images: 1477 | labels: 1477\n"
     ]
    }
   ],
   "source": [
    "train_label_dir = os.path.join(YOLO_ROOT, \"train\", \"labels\")\n",
    "val_label_dir   = os.path.join(YOLO_ROOT, \"val\", \"labels\")\n",
    "\n",
    "n_train_img = len([f for f in os.listdir(train_img_dir) if f.lower().endswith((\".jpg\",\".png\",\".jpeg\"))])\n",
    "n_train_lab = len([f for f in os.listdir(train_label_dir) if f.lower().endswith(\".txt\")])\n",
    "n_val_img   = len([f for f in os.listdir(val_img_dir) if f.lower().endswith((\".jpg\",\".png\",\".jpeg\"))])\n",
    "n_val_lab   = len([f for f in os.listdir(val_label_dir) if f.lower().endswith(\".txt\")])\n",
    "\n",
    "print(\"TRAIN images:\", n_train_img, \"| labels:\", n_train_lab)\n",
    "print(\"VAL   images:\", n_val_img,   \"| labels:\", n_val_lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa104c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images: torch.Size([4, 3, 416, 416])\n",
      "Targets[0] shape: torch.Size([1, 5])\n",
      "Sample name: 002469.jpg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "IMG_SIZE = 416\n",
    "\n",
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, img_size=416):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_size = img_size\n",
    "        self.img_files = sorted([f for f in os.listdir(img_dir)\n",
    "                                 if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + \".txt\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((self.img_size, self.img_size))\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        img = np.transpose(img, (2,0,1))\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        targets = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    targets.append(list(map(float, line.split())))\n",
    "        targets = torch.tensor(targets, dtype=torch.float32) if len(targets) else torch.zeros((0,5))\n",
    "\n",
    "        return img, targets, img_name\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, targets, names = zip(*batch)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "    return imgs, list(targets), list(names)\n",
    "\n",
    "train_ds = YoloDataset(train_img_dir, train_label_dir, IMG_SIZE)\n",
    "val_ds   = YoloDataset(val_img_dir,   val_label_dir,   IMG_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# test 1 batch\n",
    "imgs, targets, names = next(iter(train_loader))\n",
    "print(\"Batch images:\", imgs.shape)\n",
    "print(\"Targets[0] shape:\", targets[0].shape)\n",
    "print(\"Sample name:\", names[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a82fcc",
   "metadata": {},
   "source": [
    "# train YOLOv3 baseline\n",
    "Dùng dataset YOLO format đã tạo (train/images, train/labels, val/images, val/labels)\n",
    "Import model/loss từ folder 3.src \n",
    "Lưu checkpoint vào 4.Checkpoints/yolov3/\n",
    "Lưu log vào 5.Results/1.logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bfa9bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparams + class map\n",
    "NUM_CLASSES = 3\n",
    "CLASS_NAMES = [\"with_mask\", \"incorrect_mask\", \"without_mask\"]\n",
    "\n",
    "IMG_SIZE = 416\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e22f1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Dataset YOLO (đọc ảnh + label .txt) ==========\n",
    "class YoloDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Read YOLO-format labels:\n",
    "    each line: cls_id x_center y_center w h (normalized)\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, label_dir, img_size=416):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.img_files = [f for f in os.listdir(img_dir)\n",
    "                          if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "        self.img_files.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        # read image (BGR -> RGB)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # resize\n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "\n",
    "        # normalize [0,1], CHW\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        # read label\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + \".txt\")\n",
    "        targets = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    cls_id, xc, yc, w, h = map(float, line.strip().split())\n",
    "                    targets.append([cls_id, xc, yc, w, h])\n",
    "\n",
    "        if len(targets) == 0:\n",
    "            targets = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "        return img, targets, img_name\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, targets, names = [], [], []\n",
    "    for b_idx, (img, target, name) in enumerate(batch):\n",
    "        imgs.append(img)\n",
    "        names.append(name)\n",
    "\n",
    "        if target.numel() > 0:\n",
    "            # add batch index at column 0 => [batch_id, cls, xc, yc, w, h]\n",
    "            t = torch.zeros((target.size(0), 6), dtype=torch.float32)\n",
    "            t[:, 0] = b_idx\n",
    "            t[:, 1:] = target\n",
    "            targets.append(t)\n",
    "\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    targets = torch.cat(targets, dim=0) if len(targets) > 0 else torch.zeros((0, 6), dtype=torch.float32)\n",
    "    return imgs, targets, names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "362f8972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 5908\n",
      "Val samples  : 1477\n"
     ]
    }
   ],
   "source": [
    "train_ds = YoloDetectionDataset(TRAIN_IMG_DIR, TRAIN_LAB_DIR, img_size=IMG_SIZE)\n",
    "val_ds   = YoloDetectionDataset(VAL_IMG_DIR,   VAL_LAB_DIR,   img_size=IMG_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Train samples:\", len(train_ds))\n",
    "print(\"Val samples  :\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bef7e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\2. experiments\n",
      "\n",
      "Folders in PROJECT_ROOT:\n",
      " - 0. Setup\n",
      " - 1.Dataset\n",
      " - 2. experiments\n",
      " - 3. src\n",
      " - 4. Checkpoints\n",
      " - 5. Results\n",
      " - 6. Webcam\n",
      " - train\n",
      "\n",
      "SRC candidates: ['3. src']\n",
      "Detected SRC_DIR: D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\3. src\n",
      "sys.path[0]: c:\\Users\\kimli\\anaconda3\\envs\\facemask_env\\python310.zip\n",
      "\n",
      "Python files in SRC_DIR:\n",
      " - a_dataset_utils.py\n",
      " - b_model_yolo.py\n",
      " - c_model_se_yolo.py\n",
      " - d_losses.py\n",
      " - f_metrics.py\n"
     ]
    }
   ],
   "source": [
    "import os, sys, glob\n",
    "\n",
    "PROJECT_ROOT = r\"D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\"\n",
    "\n",
    "# In working dir hiện tại\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Liệt kê các folder trong PROJECT_ROOT để xem tên chính xác\n",
    "print(\"\\nFolders in PROJECT_ROOT:\")\n",
    "for x in os.listdir(PROJECT_ROOT):\n",
    "    if os.path.isdir(os.path.join(PROJECT_ROOT, x)):\n",
    "        print(\" -\", x)\n",
    "\n",
    "# Tìm folder src thật sự (có thể là '3. src' hoặc '3.src' hoặc '3_src')\n",
    "candidates = [d for d in os.listdir(PROJECT_ROOT) if \"src\" in d.lower()]\n",
    "print(\"\\nSRC candidates:\", candidates)\n",
    "\n",
    "# Chọn SRC_DIR đúng (ưu tiên folder chứa file model)\n",
    "SRC_DIR = None\n",
    "for d in candidates:\n",
    "    p = os.path.join(PROJECT_ROOT, d)\n",
    "    if os.path.isdir(p) and (glob.glob(os.path.join(p, \"*model*.py\"))):\n",
    "        SRC_DIR = p\n",
    "        break\n",
    "\n",
    "print(\"Detected SRC_DIR:\", SRC_DIR)\n",
    "\n",
    "# Add sys.path\n",
    "if SRC_DIR and SRC_DIR not in sys.path:\n",
    "    sys.path.insert(0, SRC_DIR)\n",
    "\n",
    "print(\"sys.path[0]:\", sys.path[0])\n",
    "\n",
    "# In các file .py trong src\n",
    "print(\"\\nPython files in SRC_DIR:\")\n",
    "if SRC_DIR:\n",
    "    for f in sorted(glob.glob(os.path.join(SRC_DIR, \"*.py\"))):\n",
    "        print(\" -\", os.path.basename(f))\n",
    "else:\n",
    "    print(\"Không tìm thấy folder src. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a3465ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded from: D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\3. src\\b_model_yolo.py\n",
      "Has count_parameters: True\n",
      "Available: ['YOLOv3', 'count_parameters']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import b_model_yolo\n",
    "\n",
    "importlib.reload(b_model_yolo)\n",
    "\n",
    "print(\"Reloaded from:\", b_model_yolo.__file__)\n",
    "print(\"Has count_parameters:\", hasattr(b_model_yolo, \"count_parameters\"))\n",
    "print(\"Available:\", [x for x in dir(b_model_yolo) if \"count\" in x.lower() or \"yolo\" in x.lower()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f37c5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported b_model_yolo: YOLOv3 and helpers available\n",
      "YOLOv3 -> <class 'b_model_yolo.YOLOv3'>\n",
      "yolo_loss -> yolo_loss\n"
     ]
    }
   ],
   "source": [
    "from b_model_yolo import YOLOv3, count_parameters\n",
    "from d_losses import yolo_loss\n",
    "print(\"Imported b_model_yolo: YOLOv3 and helpers available\")\n",
    "print('YOLOv3 ->', YOLOv3)\n",
    "print('yolo_loss ->', yolo_loss.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff266cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing `model` instance for parameter check.\n",
      "Number of param tensors: 33\n",
      "Total params: 4726504\n",
      "Trainable params: 4726504\n"
     ]
    }
   ],
   "source": [
    "# Safe parameter inspection: ensure `model` is an instance, not the module\n",
    "\n",
    "import types, torch\n",
    "\n",
    "def _is_module_instance(obj):\n",
    "    return isinstance(obj, torch.nn.Module)\n",
    "\n",
    "try:\n",
    "    if 'model' in globals() and _is_module_instance(globals()['model']):\n",
    "        m = globals()['model']\n",
    "        print(\"Using existing `model` instance for parameter check.\")\n",
    "    else:\n",
    "        # Try to import and instantiate a temporary model for inspection\n",
    "        from b_model_yolo import YOLOv3, count_parameters\n",
    "        print(\"`model` not present or not an nn.Module; creating a temporary YOLOv3 for inspection.\")\n",
    "        m = YOLOv3(num_classes=NUM_CLASSES, img_size=IMG_SIZE)\n",
    "\n",
    "    params = list(m.parameters())\n",
    "    print(\"Number of param tensors:\", len(params))\n",
    "\n",
    "    if len(params) > 0:\n",
    "        total = sum(p.numel() for p in params)\n",
    "        trainable = sum(p.numel() for p in params if p.requires_grad)\n",
    "        print(\"Total params:\", total)\n",
    "        print(\"Trainable params:\", trainable)\n",
    "    else:\n",
    "        print(\"❌ Model has ZERO parameters -> model_yolo.py may be registering layers incorrectly.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print('Error during parameter inspection:', type(e).__name__, str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69946a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device for model: cpu\n",
      "Trainable params: 4726504\n",
      "out[0] shape = torch.Size([2, 24, 26, 26])\n",
      "out[1] shape = torch.Size([2, 24, 52, 52])\n",
      "out[2] shape = torch.Size([2, 24, 104, 104])\n",
      "out[0] shape = torch.Size([2, 24, 26, 26])\n",
      "out[1] shape = torch.Size([2, 24, 52, 52])\n",
      "out[2] shape = torch.Size([2, 24, 104, 104])\n"
     ]
    }
   ],
   "source": [
    "from b_model_yolo import YOLOv3, count_parameters\n",
    "\n",
    "# Instantiate and move to device\n",
    "model = YOLOv3(num_classes=NUM_CLASSES, img_size=IMG_SIZE)\n",
    "model = model.to(device)\n",
    "\n",
    "# Quick sanity checks: parameter count and forward pass shapes\n",
    "print('Device for model:', next(model.parameters()).device if len(list(model.parameters())) else device)\n",
    "print('Trainable params:', count_parameters(model))\n",
    "\n",
    "import torch\n",
    "imgs = torch.randn(min(2, BATCH_SIZE), 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "outs = model(imgs)\n",
    "for i, o in enumerate(outs):\n",
    "    print(f'out[{i}] shape =', o.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d1d6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "IMG_SIZE = 416\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# (tuỳ chọn) scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fc2ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, (imgs, targets, names) in enumerate(train_loader, start=1):\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)    \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        loss = yolo_loss(outputs, targets)   # có thể là yolo_loss(outputs, targets, IMG_SIZE, ...)\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | step {step}/{len(train_loader)} | loss {loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / max(1, len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3fc7b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate (val loss)\n",
    "@torch.no_grad()\n",
    "def validate_one_epoch(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for imgs, targets, names in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        loss = yolo_loss(outputs, targets)  \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / max(1, len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b4b4ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | step 10/739 | loss 0.0000\n",
      "Epoch 1 | step 20/739 | loss 0.0000\n",
      "Epoch 1 | step 20/739 | loss 0.0000\n",
      "Epoch 1 | step 30/739 | loss 0.0000\n",
      "Epoch 1 | step 30/739 | loss 0.0000\n",
      "Epoch 1 | step 40/739 | loss 0.0000\n",
      "Epoch 1 | step 40/739 | loss 0.0000\n",
      "Epoch 1 | step 50/739 | loss 0.0000\n",
      "Epoch 1 | step 50/739 | loss 0.0000\n",
      "Epoch 1 | step 60/739 | loss 0.0000\n",
      "Epoch 1 | step 60/739 | loss 0.0000\n",
      "Epoch 1 | step 70/739 | loss 0.0000\n",
      "Epoch 1 | step 70/739 | loss 0.0000\n",
      "Epoch 1 | step 80/739 | loss 0.0000\n",
      "Epoch 1 | step 80/739 | loss 0.0000\n",
      "Epoch 1 | step 90/739 | loss 0.0000\n",
      "Epoch 1 | step 90/739 | loss 0.0000\n",
      "Epoch 1 | step 100/739 | loss 0.0000\n",
      "Epoch 1 | step 100/739 | loss 0.0000\n",
      "Epoch 1 | step 110/739 | loss 0.0000\n",
      "Epoch 1 | step 110/739 | loss 0.0000\n",
      "Epoch 1 | step 120/739 | loss 0.0000\n",
      "Epoch 1 | step 120/739 | loss 0.0000\n",
      "Epoch 1 | step 130/739 | loss 0.0000\n",
      "Epoch 1 | step 130/739 | loss 0.0000\n",
      "Epoch 1 | step 140/739 | loss 0.0000\n",
      "Epoch 1 | step 140/739 | loss 0.0000\n",
      "Epoch 1 | step 150/739 | loss 0.0000\n",
      "Epoch 1 | step 150/739 | loss 0.0000\n",
      "Epoch 1 | step 160/739 | loss 0.0000\n",
      "Epoch 1 | step 160/739 | loss 0.0000\n",
      "Epoch 1 | step 170/739 | loss 0.0000\n",
      "Epoch 1 | step 170/739 | loss 0.0000\n",
      "Epoch 1 | step 180/739 | loss 0.0000\n",
      "Epoch 1 | step 180/739 | loss 0.0000\n",
      "Epoch 1 | step 190/739 | loss 0.0000\n",
      "Epoch 1 | step 190/739 | loss 0.0000\n",
      "Epoch 1 | step 200/739 | loss 0.0000\n",
      "Epoch 1 | step 200/739 | loss 0.0000\n",
      "Epoch 1 | step 210/739 | loss 0.0000\n",
      "Epoch 1 | step 210/739 | loss 0.0000\n",
      "Epoch 1 | step 220/739 | loss 0.0000\n",
      "Epoch 1 | step 220/739 | loss 0.0000\n",
      "Epoch 1 | step 230/739 | loss 0.0000\n",
      "Epoch 1 | step 230/739 | loss 0.0000\n",
      "Epoch 1 | step 240/739 | loss 0.0000\n",
      "Epoch 1 | step 240/739 | loss 0.0000\n",
      "Epoch 1 | step 250/739 | loss 0.0000\n",
      "Epoch 1 | step 250/739 | loss 0.0000\n",
      "Epoch 1 | step 260/739 | loss 0.0000\n",
      "Epoch 1 | step 260/739 | loss 0.0000\n",
      "Epoch 1 | step 270/739 | loss 0.0000\n",
      "Epoch 1 | step 270/739 | loss 0.0000\n",
      "Epoch 1 | step 280/739 | loss 0.0000\n",
      "Epoch 1 | step 280/739 | loss 0.0000\n",
      "Epoch 1 | step 290/739 | loss 0.0000\n",
      "Epoch 1 | step 290/739 | loss 0.0000\n",
      "Epoch 1 | step 300/739 | loss 0.0000\n",
      "Epoch 1 | step 300/739 | loss 0.0000\n",
      "Epoch 1 | step 310/739 | loss 0.0001\n",
      "Epoch 1 | step 310/739 | loss 0.0001\n",
      "Epoch 1 | step 320/739 | loss 0.0000\n",
      "Epoch 1 | step 320/739 | loss 0.0000\n",
      "Epoch 1 | step 330/739 | loss 0.0000\n",
      "Epoch 1 | step 330/739 | loss 0.0000\n",
      "Epoch 1 | step 340/739 | loss 0.0000\n",
      "Epoch 1 | step 340/739 | loss 0.0000\n",
      "Epoch 1 | step 350/739 | loss 0.0000\n",
      "Epoch 1 | step 350/739 | loss 0.0000\n",
      "Epoch 1 | step 360/739 | loss 0.0000\n",
      "Epoch 1 | step 360/739 | loss 0.0000\n",
      "Epoch 1 | step 370/739 | loss 0.0000\n",
      "Epoch 1 | step 370/739 | loss 0.0000\n",
      "Epoch 1 | step 380/739 | loss 0.0000\n",
      "Epoch 1 | step 380/739 | loss 0.0000\n",
      "Epoch 1 | step 390/739 | loss 0.0000\n",
      "Epoch 1 | step 390/739 | loss 0.0000\n",
      "Epoch 1 | step 400/739 | loss 0.0000\n",
      "Epoch 1 | step 400/739 | loss 0.0000\n",
      "Epoch 1 | step 410/739 | loss 0.0000\n",
      "Epoch 1 | step 410/739 | loss 0.0000\n",
      "Epoch 1 | step 420/739 | loss 0.0000\n",
      "Epoch 1 | step 420/739 | loss 0.0000\n",
      "Epoch 1 | step 430/739 | loss 0.0000\n",
      "Epoch 1 | step 430/739 | loss 0.0000\n",
      "Epoch 1 | step 440/739 | loss 0.0000\n",
      "Epoch 1 | step 440/739 | loss 0.0000\n",
      "Epoch 1 | step 450/739 | loss 0.0000\n",
      "Epoch 1 | step 450/739 | loss 0.0000\n",
      "Epoch 1 | step 460/739 | loss 0.0000\n",
      "Epoch 1 | step 460/739 | loss 0.0000\n",
      "Epoch 1 | step 470/739 | loss 0.0000\n",
      "Epoch 1 | step 470/739 | loss 0.0000\n",
      "Epoch 1 | step 480/739 | loss 0.0000\n",
      "Epoch 1 | step 480/739 | loss 0.0000\n",
      "Epoch 1 | step 490/739 | loss 0.0000\n",
      "Epoch 1 | step 490/739 | loss 0.0000\n",
      "Epoch 1 | step 500/739 | loss 0.0000\n",
      "Epoch 1 | step 500/739 | loss 0.0000\n",
      "Epoch 1 | step 510/739 | loss 0.0000\n",
      "Epoch 1 | step 510/739 | loss 0.0000\n",
      "Epoch 1 | step 520/739 | loss 0.0000\n",
      "Epoch 1 | step 520/739 | loss 0.0000\n",
      "Epoch 1 | step 530/739 | loss 0.0000\n",
      "Epoch 1 | step 530/739 | loss 0.0000\n",
      "Epoch 1 | step 540/739 | loss 0.0000\n",
      "Epoch 1 | step 540/739 | loss 0.0000\n",
      "Epoch 1 | step 550/739 | loss 0.0000\n",
      "Epoch 1 | step 550/739 | loss 0.0000\n",
      "Epoch 1 | step 560/739 | loss 0.0000\n",
      "Epoch 1 | step 560/739 | loss 0.0000\n",
      "Epoch 1 | step 570/739 | loss 0.0000\n",
      "Epoch 1 | step 570/739 | loss 0.0000\n",
      "Epoch 1 | step 580/739 | loss 0.0000\n",
      "Epoch 1 | step 580/739 | loss 0.0000\n",
      "Epoch 1 | step 590/739 | loss 0.0000\n",
      "Epoch 1 | step 590/739 | loss 0.0000\n",
      "Epoch 1 | step 600/739 | loss 0.0000\n",
      "Epoch 1 | step 600/739 | loss 0.0000\n",
      "Epoch 1 | step 610/739 | loss 0.0000\n",
      "Epoch 1 | step 610/739 | loss 0.0000\n",
      "Epoch 1 | step 620/739 | loss 0.0000\n",
      "Epoch 1 | step 620/739 | loss 0.0000\n",
      "Epoch 1 | step 630/739 | loss 0.0000\n",
      "Epoch 1 | step 630/739 | loss 0.0000\n",
      "Epoch 1 | step 640/739 | loss 0.0000\n",
      "Epoch 1 | step 640/739 | loss 0.0000\n",
      "Epoch 1 | step 650/739 | loss 0.0000\n",
      "Epoch 1 | step 650/739 | loss 0.0000\n",
      "Epoch 1 | step 660/739 | loss 0.0000\n",
      "Epoch 1 | step 660/739 | loss 0.0000\n",
      "Epoch 1 | step 670/739 | loss 0.0000\n",
      "Epoch 1 | step 670/739 | loss 0.0000\n",
      "Epoch 1 | step 680/739 | loss 0.0000\n",
      "Epoch 1 | step 680/739 | loss 0.0000\n",
      "Epoch 1 | step 690/739 | loss 0.0000\n",
      "Epoch 1 | step 690/739 | loss 0.0000\n",
      "Epoch 1 | step 700/739 | loss 0.0000\n",
      "Epoch 1 | step 700/739 | loss 0.0000\n",
      "Epoch 1 | step 710/739 | loss 0.0000\n",
      "Epoch 1 | step 710/739 | loss 0.0000\n",
      "Epoch 1 | step 720/739 | loss 0.0000\n",
      "Epoch 1 | step 720/739 | loss 0.0000\n",
      "Epoch 1 | step 730/739 | loss 0.0000\n",
      "Epoch 1 | step 730/739 | loss 0.0000\n",
      "[Epoch 1/1] train_loss=0.0000 | val_loss=0.0000 | lr=0.000100 | time=2081.8s\n",
      " Saved BEST to: D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\4. Checkpoints\\yolov3\\best.pt\n",
      " Saved log to: D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\5. Results\\1. logs\\yolov3_baseline_log.csv\n",
      "[Epoch 1/1] train_loss=0.0000 | val_loss=0.0000 | lr=0.000100 | time=2081.8s\n",
      " Saved BEST to: D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\4. Checkpoints\\yolov3\\best.pt\n",
      " Saved log to: D:\\10.CAO HOC\\2.MON HOC TREN TRUONG\\2. HK1 2526\\4. Applications of Machine Learning in Data Analysis\\3.Project FaceMask\\Dataset_Facemask_Yolo\\5. Results\\1. logs\\yolov3_baseline_log.csv\n"
     ]
    }
   ],
   "source": [
    "#Training loop + lưu checkpoint best/last + log CSV\n",
    "import os, time\n",
    "import pandas as pd\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_loss = float(train_one_epoch(epoch))\n",
    "    val_loss   = float(validate_one_epoch(epoch))\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    lr_now = float(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    print(f\"[Epoch {epoch}/{EPOCHS}] train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | lr={lr_now:.6f} | time={dt:.1f}s\")\n",
    "\n",
    "    history.append({\n",
    "        \"epoch\": int(epoch),\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"lr\": lr_now\n",
    "    })\n",
    "\n",
    "    # ===== SAVE LAST (SAFE) =====\n",
    "    last_path = os.path.join(CKPT_DIR, \"last.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": int(epoch),\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "    }, last_path)\n",
    "\n",
    "    # ===== SAVE BEST (SAFE) =====\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_path = os.path.join(CKPT_DIR, \"best.pt\")\n",
    "        torch.save({\n",
    "            \"epoch\": int(epoch),\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "        }, best_path)\n",
    "        print(\" Saved BEST to:\", best_path)\n",
    "\n",
    "# ===== SAVE LOGS =====\n",
    "log_csv = os.path.join(LOG_DIR, \"yolov3_baseline_log.csv\")\n",
    "pd.DataFrame(history).to_csv(log_csv, index=False)\n",
    "print(\" Saved log to:\", log_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376d9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facemask_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
